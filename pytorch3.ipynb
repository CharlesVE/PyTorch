{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train = True, download = True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train = False, download = True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below we import 2 modules from pytorch. \n",
    "nn - this is to define what type of neural network we want to use i.e. convolutional, recurrent, regular fully connected layers etc. We are using fully connected layers. \n",
    "F - This saves us having to create our own relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our goal for our output is to make 3 layers comprising of 64 neurons for our hidden layers\n",
    "for every fully connected layer we have input (what's coming into that layer) the output can be whatever we want\n",
    "\n",
    "Below we have created a class Net that will inheret from the nn.Module class.\n",
    "\n",
    "While the nn.Module class is the parent class, usually it's init function will not be run by the corresponding class that uses it. \n",
    "In order to address this problem, the super().__init__() line ensures that we do actually run the parent class's init function. \n",
    "\n",
    "Additionally, from one layer to the next it is worth noting that each subsequent layer takes the previous layer's output as it's input. \n",
    "So the line self.fc2 = nn.Linear(64,64) takes the left 64 (from the self.fc1's right 64) and outputs 64 into the layer below's input.\n",
    "\n",
    "Note: the input to the self.fc1 is written as 28*28, instead of 784, only to illustrate that it is taken from a grid of 28*28 pixels. Therefore our first layer will be a flattened array of 1x784.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # super corresponds to nn.Module, it runs the initialization for nn.Module, as well as whatever we put in init\n",
    "        # Define the fully connected layers of the neural network\n",
    "        self.fc1 =nn.Linear(28*28, 64)#fc = fully connected, linear(input,output), input = 28*28\n",
    "        self.fc2 =nn.Linear(64, 64)# This layer's input later must be the one above's output layer\n",
    "        self.fc3 =nn.Linear(64, 64)\n",
    "        self.fc4 =nn.Linear(64, 10)\n",
    "        # Define how data passes through the neural network \n",
    "    def forward(self, x):\n",
    "        # x will pass through all the layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1) #dim refers to the dimension in which softmax will be computed so that all values outputted from in that particular dimension will sum to 1.\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The simplest neural network you can have is a fully connected feed forward network. In order to illustrate this, we have created the forward function above. \n",
    "Note how the forward function passes each x value of data through the relu function. This is our activation function that works to constrain the values into a sensible range. \n",
    "\n",
    "For the output layer above we have used the softmax function. \n",
    "Why not use the sigmoid function that is similar? Well the problem with the sigmoid function is that it's output does not sum to 1. This is illogical if we're thinking about mutually exclusive probabilities. \n",
    "Therefore we use the softmax function that ensures that if we sum all our output values they equal 1.\n",
    "Additionally, the softmax function accepts an entire vector rather than a singular value.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using a random image we can actually pass some data through our neural network.\n",
    "First let's create a random image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9501e-01, 7.1088e-01, 5.2206e-01, 7.4057e-01, 4.8570e-01, 1.5526e-01,\n",
       "         5.4399e-01, 9.8780e-01, 5.8096e-01, 9.6049e-01, 1.5963e-01, 9.7666e-01,\n",
       "         2.1612e-01, 6.6098e-01, 3.3898e-01, 1.9551e-01, 5.2789e-01, 3.0167e-01,\n",
       "         6.4826e-01, 4.8363e-01, 9.9652e-01, 5.9988e-01, 9.1170e-01, 4.3424e-01,\n",
       "         7.9500e-01, 9.4443e-01, 3.1812e-01, 1.8184e-01],\n",
       "        [9.7863e-01, 8.6262e-01, 9.3210e-01, 6.8865e-02, 1.3200e-01, 3.8028e-02,\n",
       "         3.8283e-01, 9.4135e-01, 8.6488e-01, 1.5392e-01, 4.9535e-01, 2.4707e-01,\n",
       "         7.5879e-02, 5.7576e-02, 6.7559e-02, 8.8180e-01, 5.5681e-01, 5.6717e-01,\n",
       "         5.5765e-01, 1.1624e-02, 6.5851e-01, 1.1834e-01, 8.6071e-01, 8.3153e-01,\n",
       "         9.3546e-01, 4.3247e-01, 9.1870e-01, 3.6396e-01],\n",
       "        [9.5202e-01, 8.2847e-01, 4.0946e-01, 2.1590e-01, 6.4542e-01, 3.1368e-01,\n",
       "         7.0108e-01, 6.8886e-01, 7.5934e-01, 3.3666e-01, 1.1312e-01, 8.4921e-01,\n",
       "         8.4619e-01, 8.1438e-01, 7.4485e-02, 5.9755e-01, 4.9087e-01, 9.0228e-01,\n",
       "         6.2677e-01, 5.9565e-01, 9.7462e-01, 2.3764e-01, 4.9593e-01, 6.4414e-01,\n",
       "         8.0920e-01, 9.4495e-03, 5.9454e-01, 2.5599e-01],\n",
       "        [8.8495e-01, 6.6712e-01, 1.8603e-01, 4.8272e-01, 8.9659e-01, 6.0175e-01,\n",
       "         2.1293e-01, 2.1552e-01, 9.1727e-01, 1.4168e-01, 9.9306e-01, 7.1733e-01,\n",
       "         1.8466e-01, 4.0913e-01, 9.8874e-01, 5.9242e-01, 6.1491e-01, 8.3971e-01,\n",
       "         4.3746e-01, 7.2104e-02, 3.4841e-01, 7.6540e-01, 1.6836e-01, 5.5413e-01,\n",
       "         4.7026e-01, 6.9099e-01, 2.1657e-01, 7.4983e-01],\n",
       "        [1.1887e-01, 1.8579e-01, 9.2562e-01, 6.3351e-01, 7.0690e-01, 9.4047e-01,\n",
       "         9.8921e-01, 8.5401e-01, 1.6407e-01, 1.3621e-02, 7.9066e-01, 4.3136e-01,\n",
       "         4.4565e-01, 7.4644e-01, 9.8886e-01, 2.0564e-01, 1.0885e-02, 2.7266e-01,\n",
       "         9.6678e-01, 8.4365e-01, 4.5021e-02, 9.3337e-01, 6.1624e-01, 3.5817e-01,\n",
       "         6.6497e-01, 9.7220e-01, 2.0932e-01, 7.4555e-01],\n",
       "        [6.4747e-01, 3.7763e-01, 1.4379e-01, 6.1453e-01, 8.8132e-01, 8.5285e-01,\n",
       "         1.3992e-01, 8.2220e-01, 5.6997e-01, 9.5934e-01, 3.2183e-01, 7.8331e-01,\n",
       "         6.1621e-01, 2.5682e-01, 9.5567e-02, 5.2838e-01, 3.7967e-01, 6.4345e-01,\n",
       "         7.9736e-01, 7.8658e-01, 4.0847e-01, 7.6373e-01, 2.8862e-01, 2.6512e-01,\n",
       "         5.2987e-01, 3.1921e-01, 9.7999e-01, 1.8493e-01],\n",
       "        [5.9720e-01, 5.5336e-02, 4.5599e-01, 9.0257e-01, 8.2055e-01, 8.2998e-01,\n",
       "         1.6218e-01, 9.4276e-01, 9.1599e-01, 9.7057e-01, 5.0846e-01, 5.1551e-01,\n",
       "         6.7615e-01, 4.7582e-01, 5.0830e-02, 1.0047e-01, 5.1018e-01, 4.6655e-01,\n",
       "         4.2468e-01, 9.2468e-01, 7.5561e-01, 2.6399e-02, 3.2773e-01, 9.7545e-01,\n",
       "         8.4032e-01, 6.7858e-01, 3.8132e-01, 4.3726e-01],\n",
       "        [4.2628e-01, 3.1361e-01, 7.2101e-01, 2.0453e-01, 3.2696e-01, 3.3296e-01,\n",
       "         9.9540e-01, 6.3107e-01, 3.0078e-01, 7.6451e-01, 8.8394e-02, 3.3079e-01,\n",
       "         5.9698e-01, 6.2567e-01, 7.0469e-02, 5.6620e-01, 5.9746e-01, 9.0088e-01,\n",
       "         2.1691e-01, 1.4672e-01, 4.0535e-01, 8.8697e-01, 5.2178e-01, 8.5966e-01,\n",
       "         5.7523e-01, 9.5111e-01, 7.4179e-01, 2.2032e-01],\n",
       "        [1.4841e-01, 2.6606e-01, 8.1403e-01, 6.0415e-01, 9.5738e-01, 6.7564e-01,\n",
       "         8.4220e-01, 5.4613e-01, 8.4650e-01, 7.8769e-01, 5.3609e-01, 6.0823e-01,\n",
       "         1.5262e-01, 3.9086e-01, 9.7584e-01, 7.7411e-01, 8.6974e-03, 1.8010e-01,\n",
       "         9.9776e-03, 2.1124e-01, 4.0928e-02, 6.4899e-01, 1.2116e-01, 5.0060e-01,\n",
       "         7.1984e-01, 9.8288e-01, 3.6633e-01, 3.1046e-01],\n",
       "        [8.3167e-01, 9.7557e-01, 7.9487e-01, 1.0525e-01, 1.7621e-02, 6.6441e-01,\n",
       "         1.1694e-01, 4.6585e-01, 3.7604e-01, 2.8445e-02, 8.1172e-01, 1.8893e-01,\n",
       "         2.4459e-01, 9.8440e-01, 6.9427e-01, 1.6840e-01, 1.5141e-01, 7.8381e-01,\n",
       "         9.2328e-01, 5.8226e-01, 4.6547e-01, 6.4792e-01, 9.5041e-01, 6.4006e-01,\n",
       "         7.0036e-01, 5.2924e-02, 1.1594e-01, 2.3892e-01],\n",
       "        [4.1698e-01, 5.0714e-01, 8.9910e-01, 8.3640e-02, 5.3838e-01, 4.0561e-01,\n",
       "         9.8454e-01, 4.8337e-01, 1.8634e-01, 6.6015e-01, 7.7439e-02, 4.5548e-01,\n",
       "         7.1963e-01, 2.4483e-01, 3.1305e-02, 7.2773e-01, 6.5811e-01, 3.8630e-01,\n",
       "         9.0419e-01, 4.8152e-02, 6.8323e-01, 2.8935e-01, 1.1311e-01, 8.8769e-03,\n",
       "         3.6494e-01, 2.1957e-01, 4.1357e-01, 9.1901e-01],\n",
       "        [3.1626e-01, 1.7473e-01, 1.7628e-01, 4.9963e-01, 3.6190e-01, 2.1310e-01,\n",
       "         4.9878e-01, 5.4388e-01, 5.7127e-01, 4.0959e-01, 9.6100e-01, 8.9684e-01,\n",
       "         8.1764e-01, 1.0204e-01, 8.6286e-01, 3.0510e-02, 8.5319e-01, 5.9834e-01,\n",
       "         7.9509e-01, 6.1770e-02, 1.2560e-01, 2.3352e-01, 7.3911e-01, 6.0066e-01,\n",
       "         6.3419e-01, 8.2248e-02, 6.3487e-01, 3.8699e-01],\n",
       "        [7.8013e-01, 1.8929e-01, 6.8692e-01, 8.2148e-02, 8.7977e-01, 5.5082e-01,\n",
       "         7.5204e-01, 7.3156e-01, 1.8192e-01, 5.4743e-02, 8.3220e-01, 5.5827e-01,\n",
       "         1.0997e-01, 8.3465e-02, 6.6461e-01, 3.8919e-01, 4.9185e-01, 3.6583e-01,\n",
       "         1.1020e-01, 6.9834e-01, 4.2512e-01, 8.4464e-01, 5.6171e-01, 6.9693e-01,\n",
       "         2.3753e-01, 4.5734e-01, 9.2784e-01, 6.7293e-01],\n",
       "        [3.3908e-01, 7.2673e-01, 5.4368e-01, 6.9166e-01, 2.6044e-02, 7.0875e-01,\n",
       "         6.8912e-01, 2.9636e-01, 8.2951e-01, 8.6632e-01, 4.0873e-01, 6.4614e-01,\n",
       "         6.2102e-01, 8.6476e-01, 4.0222e-02, 2.7187e-01, 4.6582e-02, 5.2571e-01,\n",
       "         4.9510e-01, 6.0944e-01, 6.6037e-01, 6.9972e-01, 3.4202e-01, 6.5936e-01,\n",
       "         8.7028e-01, 2.2580e-01, 4.6841e-01, 2.2022e-01],\n",
       "        [6.0320e-01, 3.9311e-01, 6.9112e-01, 7.6004e-01, 7.6394e-01, 2.9634e-01,\n",
       "         8.4190e-01, 3.4580e-01, 6.6337e-01, 6.0520e-03, 2.1198e-01, 8.2936e-01,\n",
       "         2.9884e-01, 7.9040e-02, 8.4570e-01, 3.4718e-01, 5.7417e-01, 6.6957e-01,\n",
       "         9.1828e-01, 2.0086e-01, 3.1001e-01, 4.0394e-01, 9.6579e-01, 3.8983e-01,\n",
       "         3.4324e-01, 5.5581e-01, 4.1338e-01, 5.8019e-01],\n",
       "        [8.8608e-01, 1.8915e-01, 8.6836e-02, 2.7029e-01, 7.2279e-01, 5.6138e-01,\n",
       "         6.9252e-01, 9.8012e-01, 1.6799e-01, 6.9425e-01, 6.1436e-01, 6.5929e-01,\n",
       "         9.2350e-01, 3.6137e-01, 6.8229e-02, 7.5012e-01, 9.6112e-04, 4.7831e-01,\n",
       "         3.7689e-01, 4.2938e-02, 8.5890e-01, 7.3925e-01, 4.2288e-01, 1.3303e-02,\n",
       "         8.9827e-01, 3.9898e-01, 5.5858e-01, 8.6975e-02],\n",
       "        [8.5147e-01, 9.4436e-01, 3.8897e-01, 6.0486e-01, 5.2407e-01, 3.9473e-02,\n",
       "         2.6374e-02, 8.9748e-01, 3.7876e-01, 6.3933e-01, 5.5122e-01, 7.3333e-01,\n",
       "         1.9445e-01, 2.0449e-02, 3.2671e-01, 2.5449e-01, 5.2259e-01, 2.8943e-01,\n",
       "         4.4061e-01, 5.9649e-01, 4.2598e-01, 1.9387e-01, 1.9683e-01, 1.6530e-01,\n",
       "         6.0714e-01, 5.3723e-01, 2.5311e-02, 8.9019e-01],\n",
       "        [1.4837e-01, 3.3121e-01, 3.8947e-01, 2.5715e-01, 1.0189e-01, 5.7402e-01,\n",
       "         9.5550e-01, 4.2225e-01, 4.8944e-01, 8.2414e-01, 6.9770e-01, 6.7361e-01,\n",
       "         1.1681e-01, 2.3231e-01, 1.3115e-01, 3.1755e-01, 6.2346e-01, 8.1703e-02,\n",
       "         1.9278e-01, 8.1826e-01, 4.6244e-01, 6.5345e-01, 2.6947e-01, 7.5854e-01,\n",
       "         3.0632e-01, 7.0075e-01, 7.7308e-01, 8.2267e-01],\n",
       "        [6.8833e-01, 9.8613e-01, 5.8085e-01, 1.0602e-01, 9.3970e-01, 4.0849e-01,\n",
       "         6.9045e-01, 6.2239e-01, 9.9716e-01, 4.8900e-01, 5.5683e-02, 2.8054e-01,\n",
       "         1.4090e-01, 5.3905e-01, 2.4288e-01, 1.1937e-01, 1.6992e-01, 7.2533e-01,\n",
       "         8.5938e-01, 9.2838e-02, 7.0538e-01, 1.0958e-01, 5.5491e-01, 4.6739e-01,\n",
       "         7.5038e-01, 3.9712e-01, 1.0990e-01, 5.6225e-01],\n",
       "        [3.6349e-01, 9.2226e-01, 2.9651e-01, 9.7707e-01, 4.2084e-01, 4.1812e-01,\n",
       "         6.0881e-01, 2.4465e-01, 1.7968e-01, 8.0877e-01, 8.0241e-01, 4.5450e-01,\n",
       "         8.4689e-01, 5.5786e-01, 4.6167e-01, 3.3209e-02, 7.0589e-01, 9.9901e-02,\n",
       "         7.0806e-01, 6.1684e-01, 5.3424e-01, 8.3368e-01, 9.8252e-01, 2.6229e-01,\n",
       "         4.7796e-01, 3.1670e-01, 2.3276e-01, 7.3152e-01],\n",
       "        [9.4925e-01, 9.0736e-01, 1.5975e-02, 5.3047e-01, 1.0765e-01, 5.3856e-02,\n",
       "         9.0882e-01, 8.3053e-01, 4.4357e-02, 5.9925e-01, 8.2812e-01, 4.6688e-02,\n",
       "         7.3879e-01, 7.5604e-01, 8.8328e-01, 2.3383e-01, 8.9564e-01, 3.4535e-01,\n",
       "         5.3016e-01, 4.6067e-01, 6.2747e-02, 3.1393e-01, 7.3005e-01, 6.0717e-01,\n",
       "         3.0809e-01, 5.1379e-01, 1.3421e-01, 4.3201e-01],\n",
       "        [6.9021e-01, 6.6978e-01, 1.0180e-01, 5.1117e-01, 6.7463e-01, 4.5523e-01,\n",
       "         3.2799e-01, 3.8525e-02, 8.7826e-01, 3.5694e-01, 9.4655e-01, 1.9804e-01,\n",
       "         4.5747e-02, 8.2781e-02, 7.5390e-01, 6.8735e-01, 7.5362e-02, 1.6059e-01,\n",
       "         4.5611e-01, 1.7244e-01, 6.9228e-02, 3.1049e-01, 8.5347e-01, 3.2757e-01,\n",
       "         2.6625e-01, 6.3894e-01, 3.6652e-01, 1.9626e-01],\n",
       "        [8.6914e-01, 3.5470e-01, 4.8231e-01, 9.6067e-01, 7.0367e-01, 6.4151e-01,\n",
       "         5.0031e-01, 7.7913e-01, 8.0161e-01, 5.8386e-01, 5.1950e-01, 2.0438e-01,\n",
       "         5.1442e-01, 4.8120e-01, 9.0918e-01, 9.9688e-01, 6.6181e-01, 8.5943e-01,\n",
       "         9.6151e-02, 5.2465e-01, 1.2704e-01, 3.1869e-02, 4.4790e-01, 9.5129e-01,\n",
       "         6.5476e-02, 9.7524e-01, 9.8735e-01, 3.3323e-01],\n",
       "        [5.3119e-01, 8.5830e-01, 9.4085e-01, 7.4103e-01, 7.7455e-01, 8.2867e-01,\n",
       "         8.7983e-01, 3.1689e-01, 5.4201e-02, 6.8329e-01, 1.9652e-01, 7.7708e-01,\n",
       "         1.4338e-01, 5.5328e-01, 4.2327e-01, 1.7362e-01, 4.6644e-01, 5.3348e-01,\n",
       "         1.3089e-02, 1.2131e-01, 1.1932e-01, 5.5243e-01, 3.0912e-01, 9.6386e-02,\n",
       "         2.4649e-01, 5.1530e-01, 4.6080e-01, 8.9201e-01],\n",
       "        [3.4944e-01, 1.2800e-01, 6.8484e-01, 1.7947e-01, 1.1108e-01, 3.3217e-01,\n",
       "         4.7761e-01, 9.8100e-01, 3.6031e-01, 6.9229e-01, 8.4371e-01, 4.6086e-01,\n",
       "         3.7202e-01, 9.1800e-01, 1.3376e-01, 7.2802e-01, 1.9101e-01, 4.1338e-01,\n",
       "         3.2650e-01, 2.0853e-02, 6.1435e-01, 5.2418e-01, 6.3205e-01, 4.0605e-01,\n",
       "         1.4168e-01, 1.5680e-01, 1.1698e-01, 7.4837e-01],\n",
       "        [1.7368e-01, 8.2811e-01, 1.0886e-01, 3.0744e-01, 4.6908e-01, 6.1650e-01,\n",
       "         1.3227e-01, 2.6005e-01, 4.1010e-02, 5.0757e-01, 1.4812e-01, 1.8906e-01,\n",
       "         4.9846e-01, 6.1359e-01, 7.7844e-01, 9.3129e-01, 9.4245e-01, 4.4968e-01,\n",
       "         6.2951e-01, 9.8828e-02, 8.2007e-01, 3.3336e-01, 7.7849e-01, 7.7283e-01,\n",
       "         2.8603e-01, 3.3147e-01, 5.8440e-01, 8.3216e-01],\n",
       "        [3.6922e-01, 7.6676e-02, 6.9434e-01, 5.9127e-01, 4.5131e-01, 4.9221e-01,\n",
       "         9.4050e-01, 8.5616e-01, 9.4658e-01, 4.5796e-02, 5.2283e-01, 8.9927e-01,\n",
       "         6.3996e-01, 3.5947e-01, 4.5143e-01, 3.9406e-02, 6.6714e-02, 3.3045e-01,\n",
       "         6.3376e-01, 9.7337e-01, 9.2876e-01, 5.1899e-01, 8.1666e-01, 7.3336e-01,\n",
       "         6.4838e-01, 6.1786e-01, 9.3657e-02, 9.2991e-01],\n",
       "        [8.7355e-01, 2.4373e-01, 4.7146e-01, 9.7650e-01, 7.6351e-01, 2.1333e-01,\n",
       "         3.6926e-01, 3.2486e-01, 6.2861e-02, 9.6433e-02, 3.3436e-01, 9.9049e-01,\n",
       "         6.0856e-01, 1.0884e-01, 4.9534e-01, 9.2254e-01, 2.6443e-02, 8.5093e-01,\n",
       "         6.9640e-02, 7.4162e-02, 7.6599e-01, 5.8083e-01, 2.4861e-01, 2.9604e-01,\n",
       "         5.2188e-01, 5.9834e-01, 6.2199e-01, 9.4585e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Let us suppose that we did not know the shape of the tensor for a moment. We could find out by doing...\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The -1 specifies that the input tensor shall be of any size. The reason for this is that any output or input to our neural network is expected to be a group feature sets. Even if you intend to just pass 1 set of features, you still have to pass it as a \"list\" of features. So the -1 allows us to either pass in 28*28 or 1,12,92. The variable part is how many samples we'll pass through. \n",
    "\n",
    "View flattens out X into a 1*784 matrix so that it can go into the first input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.view(-1, 28*28) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5118, -2.1562, -2.3876, -2.4152, -2.2221, -2.3060, -2.2989, -2.1857,\n",
       "         -2.2330, -2.3655]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
